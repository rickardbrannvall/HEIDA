{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c83416-fbe7-4995-9772-aaba61dcdaa7",
   "metadata": {},
   "source": [
    "### Use case example: Logistic Regression\n",
    "In Alice's work she encounter many types of problems that requires many different models. But as a clinical researcher, often times the data is sensetive. She know what type of data shes dealing with and knows the Logistic Regression is the model to use. To circumvent the data privacy issue, she can use such a model if it's implemented using Fully Homomorphic Encryption (FHE)\n",
    "Concrete ML, by Zama, is an open-source, privacy-preserving machine learning inference framework based on FHE. It allows data scientists, even those without any prior knowledge of cryptography, to convert machine learning models into their FHE equivalents using familiar APIs from scikit-learn and PyTorch.\n",
    "\n",
    "Using Concrete ML for logistic regression, Alice can follow the usual workflow of training a model on unencrypted data using scikit-learn. The model can then be quantized, to use integers during inference, as FHE operates over integers. After the quantization, the model can compiled into an FHE possible equivalent. This compiled model can then perform inference encrypted, i.e., prediction on encrypted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abd9129-2361-424c-bbc9-5b579fd5d0db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    !python3.8 -m pip install -U pip wheel setuptools\n",
    "    !python3.8 -m pip install  concrete-ml torchmetrics\n",
    "\n",
    "## Try uninstalling cublas if there's a cuda problem\n",
    "# !python3.8 -m pip uninstall -y nvidia_cublas_cu12\n",
    "# !python3.8 -m pip uninstall -y nvidia_cublas_cu11\n",
    "# !python3.8 -m pip uninstall -y nvidia_cublas_cu10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89c22ee-67f6-4ab4-b94a-769624eb1918",
   "metadata": {},
   "source": [
    "## 0.1 Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006d17c-9703-43be-984c-bba00463ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torchmetrics.classification import BinaryAccuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6eb7d2-be4f-4770-ba1c-0e44352db19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_column_index_to_integer(df):\n",
    "    df.columns = range(len(df.columns))\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"./data/myTenYearCHD_n1000.csv\", index_col=0)\n",
    "\n",
    "# Replace column (string) idexes with integer indexes (only integer indexes supported in the following examples)\n",
    "df = reset_column_index_to_integer(df)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9c01b-b17e-41b5-901d-7e66ac232017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and trim data\n",
    "X = df.iloc[:,:-1].copy()\n",
    "X0 = np.percentile(X,1, axis=0)\n",
    "X1 = np.percentile(X,99, axis=0)\n",
    "X = X.clip(X0,X1,axis=1)\n",
    "X = 2*(X-X.min())/(X.max()-X.min())-1\n",
    "X.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7f186-8bd1-4f64-8897-5954e9f37a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 800\n",
    "\n",
    "# Make training data into tensors \n",
    "X_trn = torch.tensor(X.iloc[:train_len,:].values).float()\n",
    "Y_trn = torch.tensor(df.iloc[:train_len,-1:].values).int()\n",
    "\n",
    "X_tst = torch.tensor(X.iloc[train_len:,:].values).float()\n",
    "Y_tst = torch.tensor(df.iloc[train_len:,-1:].values).float()\n",
    "\n",
    "\n",
    "n_features = X_trn.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ebce6-7b5e-4bc1-9b3a-7629418b8305",
   "metadata": {},
   "source": [
    "## 1.1 Using concrete-ml built-in Logistic Regression\n",
    "\n",
    "Alice is using a logistic regression model from the concrete-ml-sklearn module in her code. \n",
    "First, she trains the model in plain text, subsequently, Alice can make predictions in plain text.\n",
    "After that, she can also compile the model for FHE-execution.\n",
    "\n",
    "With the compiled model, Alice makes predictions on the same test data but this time with encryption. Finally, she calculates predictions the for both plain text and under encryption, along with the percentage of similarity between them to assess how well the model performs with encryption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764d3682-50c3-40e6-a797-6f7e28091029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from concrete.ml.sklearn import LogisticRegression\n",
    "\n",
    "# Train the network in plain text\n",
    "model = LogisticRegression(n_bits=7)\n",
    "model.fit(X_trn, Y_trn)\n",
    "\n",
    "# Do prediction in plain text\n",
    "y_clear = model.predict(X_tst)\n",
    "\n",
    "# Compile model for FHE execution, quantize and create encryption keys associated with the model\n",
    "fhe_model = model.compile(X_trn)\n",
    "\n",
    "# Do prediction with encryption\n",
    "y_fhe_simple = model.predict(X_tst, fhe=\"execute\")\n",
    "\n",
    "# Assess accuracy and similarity between FHE and plain text execution\n",
    "print(\"plain text: \", y_clear)\n",
    "print(\"encrypted simple: \", y_fhe_simple)\n",
    "print(f\"Similarity: {int((y_fhe_simple == y_clear).mean()*100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcf4838-bca8-4825-a8b9-a17f24f9ae92",
   "metadata": {},
   "source": [
    "## 1.2 Using concrete-ml built-in Logistic Regression\n",
    "Or Alice can partition the FHE execution to have more transparancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0343c734-ae87-490e-a099-181046d90c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_fhe_step = []\n",
    "for f_input in X_tst:\n",
    "    \n",
    "    # Quantize an input (float)\n",
    "    q_input = model.quantize_input([f_input.numpy()])\n",
    "    \n",
    "    # Encrypt the input\n",
    "    q_input_enc = fhe_model.encrypt(q_input)\n",
    "\n",
    "    # Execute the linear product in FHE \n",
    "    q_y_enc = fhe_model.run(q_input_enc)\n",
    "\n",
    "    # Decrypt the result (integer)\n",
    "    q_y = fhe_model.decrypt(q_y_enc)\n",
    "\n",
    "    # De-quantize the result\n",
    "    y = model.dequantize_output(q_y)\n",
    "\n",
    "    # Apply either the sigmoid if it is a binary classification task, which is the case in this \n",
    "    # example, or a softmax function in order to get the probabilities (in the clear)\n",
    "    y_proba = model.post_processing(y)\n",
    "\n",
    "    # Since this model does classification, apply the argmax to get the class predictions (in the clear)\n",
    "    # Note that regression models won't need the following line\n",
    "    y_class = np.argmax(y_proba, axis=1)\n",
    "\n",
    "    # Append each result\n",
    "    y_pred_fhe_step += list(y_class)\n",
    "\n",
    "y_fhe = np.array(y_pred_fhe_step)\n",
    "\n",
    "# Assess accuracy and similarity between FHE and plain text execution\n",
    "print(\"plain text: \", y_clear)\n",
    "print(\"encrypted: \", y_fhe)\n",
    "print(f\"Similarity: {int((y_fhe == y_clear).mean()*100)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4c65a-a73b-4a76-83df-1cda1c42b722",
   "metadata": {},
   "source": [
    "## 2 General solution for deep learning\n",
    "For existing pre-defined models, like the previous Logistic Regression model, Alice can easily import, train and predict.\n",
    "But for more complex and intricate models, Alice must be able to build and train a custom model.\n",
    "In the code below, she does just that. She first define her custom model, using normal Pytorch-based nn modules and trains it.\n",
    "Just like the previous example she compiles the model, this time using \"compile_torch_model\". Similar to the predefined LogisticResgression model, it requres a good representation for quantazation and the resulting model requires numpy inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b00ab1a-71e5-41ec-8602-e903db4dbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of incompability with jupyter, uncomment the line below and uncomment the next cell and run that too\n",
    "# Add a few lines of code at the bottom of this cell to do some result calculations\n",
    "\n",
    "# %%writefile custom_fhe_model.py\n",
    "\n",
    "from concrete.ml.torch.compile import compile_torch_model\n",
    "from utils import train_plain_model\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "# Epochs for training\n",
    "n_epochs = 100\n",
    "\n",
    "# Set number of bits for quantazation\n",
    "n_bits = 6\n",
    "\n",
    "# Define the model to use\n",
    "class LR(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.lr = nn.Linear(n_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.lr(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# The data to use for input in the quantization of the model\n",
    "torch_input = X_trn\n",
    "\n",
    "# Train model on available training data\n",
    "torch_model = train_plain_model(X_trn, Y_trn, LR(n_features), n_epochs=n_epochs, verbose=False)\n",
    "\n",
    "# Quantize and compile the trained torch model for FHE-inference\n",
    "quantized_numpy_module = compile_torch_model(\n",
    "    torch_model,\n",
    "    torch_input,\n",
    "    n_bits=n_bits,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff7a16-d99d-4085-ab55-1674081f79cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case of jupyter incopmapability, uncomment and run what's below\n",
    "# %run custom_fhe_model.py\n",
    "# OR \n",
    "# !python3.8 custom_fhe_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb8613-9451-4e48-bb82-386143a53f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Run the original model with plain text\n",
    "y_plain_pred = []\n",
    "for x_test in tqdm(X_tst):\n",
    "    y_plain_pred.append(torch_model(x_test.to(torch.float32)).detach().numpy)\n",
    "\n",
    "# Run quantized model under encryption\n",
    "y_fhe_pred = []\n",
    "for x_test in tqdm(X_tst):\n",
    "    y_fhe_pred.append(quantized_numpy_module.forward(x_test.unsqueeze(0).numpy, fhe=\"execute\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88614d7-9704-48aa-bbc5-8be3cbdd1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rmse  function\n",
    "rmse = lambda x: (x**2).mean()**0.5\n",
    "\n",
    "# Calculate the root mean square error between the original model prediction and the encrypted prediction\n",
    "RMSE = rmse(np.asarray(y_plain_pred) - np.asarray(y_fhe_pred))\n",
    "print('RMSE: ', RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebdb6b-07ce-4696-996f-800c8d2a53f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
